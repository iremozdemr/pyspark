{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e961f7d-1215-43eb-ad89-ec3c66ff41d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/10 12:44:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# import libraries from pyspark \n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# set values for Spark configuration\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"Tutorial\")\n",
    "\n",
    "# get (if already running) or create a Spark Context\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "947d3c6e-fa7d-4da0-a05e-b3246c34d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries from pyspark \n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# set values for Spark configuration\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"Tutorial\")\n",
    "\n",
    "# get (if already running) or create a Spark Context\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25bf4481-43c1-4e18-8227-57bee9800c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.master', 'local')\n",
      "('spark.app.startTime', '1739180669046')\n",
      "('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.driver.port', '49977')\n",
      "('spark.app.submitTime', '1739180668893')\n",
      "('spark.app.id', 'local-1739180669398')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.driver.host', 'mac.modem.local')\n",
      "('spark.app.name', 'Tutorial')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.ui.showConsoleProgress', 'true')\n"
     ]
    }
   ],
   "source": [
    "# check (try) if SparkContext variable (sc) exists and print information about the SparkContext\n",
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    print(\"please create SparkContext first (run cell above)\")\n",
    "else:\n",
    "    configurations = sc.getConf().getAll()\n",
    "    for item in configurations: print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a0b7dda-5b20-4f8c-8b32-561d456c79c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mac.modem.local:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Tutorial</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=Tutorial>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da9708c-f0e0-49c3-97f6-9eb9142ddb71",
   "metadata": {},
   "source": [
    "# creating rdds:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f27feb5-36f7-4566-9b30-5e5cfc0a0cba",
   "metadata": {},
   "source": [
    "### the first way to create an rdd\n",
    "\n",
    "### is to parallelize an python object\n",
    "\n",
    "### meaning converting it to a distributed dataset that can be operated in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fffc873-331e-48e9-a25e-df52b5d3d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of strings\n",
    "stringList = [\"spark is awesome\",\"spark is cool\"]\n",
    "\n",
    "# convert list of strings into a spark rdd\n",
    "stringRDD = sc.parallelize(stringList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b6b4cc0-b6e7-4fcb-b402-ffbffa6c8106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output rdd information\n",
    "stringRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732e219a-d2fa-4ea2-b8eb-6ea4a7b10f90",
   "metadata": {},
   "source": [
    "one thing to notice is that you are not able to see the output\n",
    "\n",
    "because of spark's lazy evaluation untill you call an action on that rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "103e59fc-5f25-4713-a3dc-e497a21d608b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark is awesome', 'spark is cool']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve all the elements of the rdd/dataFrame/dataset (from all nodes)\n",
    "stringRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e19d2be-bbc2-4d23-9af5-af720c368590",
   "metadata": {},
   "source": [
    ".collect() is an action \n",
    "\n",
    "as it name suggests it collects all the rows from each of the partitions in an rdd \n",
    "\n",
    "and brings them over to the driver program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c32cd9-ccb0-4112-9f38-eeb00ee57972",
   "metadata": {},
   "source": [
    "### the second way to create an RDD is \n",
    "\n",
    "### to read a dataset from a storage system which can be a \n",
    "\n",
    "### local computer file system\n",
    "\n",
    "### hdfs \n",
    "\n",
    "### cassandra\n",
    "\n",
    "### amazon s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0c3a52a-4127-470d-a799-fce06636d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read text file into rdd\n",
    "ratings = sc.textFile(\"./ratings.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2edc10d4-d431-4342-929a-793e688f5247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1::1193::5::978300760',\n",
       " '1::661::3::978302109',\n",
       " '1::914::3::978301968',\n",
       " '1::3408::4::978300275',\n",
       " '1::2355::5::978824291']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve all the elements of the rdd/dataFrame/dataset (from all nodes) and output first 5 rows\n",
    "ratings.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5971877b-b8dd-4d37-b98a-41fd9fa69807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1::1193::5::978300760',\n",
       " '1::661::3::978302109',\n",
       " '1::914::3::978301968',\n",
       " '1::3408::4::978300275',\n",
       " '1::2355::5::978824291']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the first 5 elements of the rdd\n",
    "ratings.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b04686e",
   "metadata": {},
   "source": [
    "# actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b38af",
   "metadata": {},
   "source": [
    "### get num partitions action:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90859cd2",
   "metadata": {},
   "source": [
    "return the number of partitions in rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a4aadbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[[1], [2, 3]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# create rdd 'x' with two partitions\n",
    "x = sc.parallelize([1,2,3],2)\n",
    "\n",
    "numPartitions = x.getNumPartitions()\n",
    "\n",
    "# glom() flattens elements on the same partition\n",
    "print(x.collect())\n",
    "print(x.glom().collect())\n",
    "print(numPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5a35ad",
   "metadata": {},
   "source": [
    "## collect action:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9154d9e4",
   "metadata": {},
   "source": [
    "return all items in the rdd to the driver in a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db953e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3]]\n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# create rdd 'x' with two partitions\n",
    "x = sc.parallelize([1,2,3],2)\n",
    "\n",
    "print(x.glom().collect())\n",
    "print(x.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e541bcae",
   "metadata": {},
   "source": [
    "## count action:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57065e1",
   "metadata": {},
   "source": [
    "return the number of elements in this rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a359f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new rdd 'numberRDD' with two partitions\n",
    "numberRDD = sc.parallelize([1,2,3,4,5,6,7,8,9,10],2)\n",
    "\n",
    "# the action count returns the number of element in the dataset - independent of the number of partitions \n",
    "numberRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb0b371",
   "metadata": {},
   "source": [
    "## first action:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b497fd",
   "metadata": {},
   "source": [
    "return the first element in this rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cdd846a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new rdd 'numberRDD' with two partitions\n",
    "numberRDD = sc.parallelize([1,2,3,4,5,6,7,8,9,10],2)\n",
    "\n",
    "# return the first element - the order of the elements within the rdd is not effected by the partitioning\n",
    "numberRDD.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319bd7ef",
   "metadata": {},
   "source": [
    "## take action:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e8bae0",
   "metadata": {},
   "source": [
    "take the first num elements of the rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a27c3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new rdd 'numberRDD' with two partitions\n",
    "numberRDD = sc.parallelize([1,2,3,4,5,6,7,8,9,10],2)\n",
    "\n",
    "# return the first four element - the order of the elements within the rdd is not effected by the partitioning\n",
    "numberRDD.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeafdd2",
   "metadata": {},
   "source": [
    "## reduce action:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b058df",
   "metadata": {},
   "source": [
    "aggregate all the elements of the rdd \n",
    "\n",
    "by applying a user function pairwise to elements and partial results\n",
    "\n",
    "and returns a result to the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8957d916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# create new rdd 'x'\n",
    "x = sc.parallelize([1,2,3,4])\n",
    "\n",
    "# apply function pairwise (a,b) to elements and return the sum - return type is integer\n",
    "y = x.reduce(lambda a,b: a+b)\n",
    "\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a3ba91",
   "metadata": {},
   "source": [
    "## aggregate action:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71cbae6",
   "metadata": {},
   "source": [
    "since rdds are partitioned\n",
    "\n",
    "the aggregate takes full advantage of it \n",
    "\n",
    "by first aggregating elements in each partition \n",
    "\n",
    "and then aggregating results of all partition to get the final result\n",
    "\n",
    "aggregate all the elements of the rdd by:\n",
    "\n",
    "- applying a user function to combine elements with user-supplied objects\n",
    "\n",
    "- then combining those user-defined results via a second user function\n",
    "    \n",
    "- and finally returning a result to the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa348c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 2, 3, 4], 10)\n"
     ]
    }
   ],
   "source": [
    "# the seqOp operator is used to accumulate the results of each partition and stores the running \n",
    "# accumulated result to data\n",
    "seqOp = lambda data, item: (data[0] + [item], data[1] + item)\n",
    "\n",
    "# The combOp is used to combine the results of all partitions\n",
    "combOp = lambda d1, d2: (d1[0] + d2[0], d1[1] + d2[1])\n",
    "\n",
    "# create new rdd 'x'\n",
    "x = sc.parallelize([1,2,3,4])\n",
    "\n",
    "# aggregate all elements of the rdd\n",
    "y = x.aggregate(([], 0), seqOp, combOp)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b143118a",
   "metadata": {},
   "source": [
    "## max action:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e744bf",
   "metadata": {},
   "source": [
    "return the maximum item in the rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6eebc17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new rdd 'x'\n",
    "x = sc.parallelize([1,2,4,3])\n",
    "\n",
    "# return the maximum value from the dataset\n",
    "x.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96d533c",
   "metadata": {},
   "source": [
    "# stop the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca3ea281",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    print(\"spark context does not context exist - nothing to stop\")\n",
    "else:\n",
    "    sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myproject_kernel",
   "language": "python",
   "name": "myproject_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
