{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e961f7d-1215-43eb-ad89-ec3c66ff41d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/10 11:59:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# import libraries from pyspark \n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# set values for Spark configuration\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"Tutorial\")\n",
    "\n",
    "# get (if already running) or create a Spark Context\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "947d3c6e-fa7d-4da0-a05e-b3246c34d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries from pyspark \n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# set values for Spark configuration\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"Tutorial\")\n",
    "\n",
    "# get (if already running) or create a Spark Context\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25bf4481-43c1-4e18-8227-57bee9800c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.master', 'local')\n",
      "('spark.app.startTime', '1739177942173')\n",
      "('spark.app.submitTime', '1739177942016')\n",
      "('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.driver.host', 'mac.modem.local')\n",
      "('spark.app.name', 'Tutorial')\n",
      "('spark.driver.port', '49508')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.app.id', 'local-1739177942558')\n",
      "('spark.ui.showConsoleProgress', 'true')\n"
     ]
    }
   ],
   "source": [
    "# check (try) if SparkContext variable (sc) exists and print information about the SparkContext\n",
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    print(\"please create SparkContext first (run cell above)\")\n",
    "else:\n",
    "    configurations = sc.getConf().getAll()\n",
    "    for item in configurations: print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a0b7dda-5b20-4f8c-8b32-561d456c79c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mac.modem.local:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Tutorial</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=Tutorial>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da9708c-f0e0-49c3-97f6-9eb9142ddb71",
   "metadata": {},
   "source": [
    "# creating rdds:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f27feb5-36f7-4566-9b30-5e5cfc0a0cba",
   "metadata": {},
   "source": [
    "### the first way to create an rdd\n",
    "\n",
    "### is to parallelize an python object\n",
    "\n",
    "### meaning converting it to a distributed dataset that can be operated in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fffc873-331e-48e9-a25e-df52b5d3d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of strings\n",
    "stringList = [\"spark is awesome\",\"spark is cool\"]\n",
    "\n",
    "# convert list of strings into a spark rdd\n",
    "stringRDD = sc.parallelize(stringList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b6b4cc0-b6e7-4fcb-b402-ffbffa6c8106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output rdd information\n",
    "stringRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732e219a-d2fa-4ea2-b8eb-6ea4a7b10f90",
   "metadata": {},
   "source": [
    "one thing to notice is that you are not able to see the output\n",
    "\n",
    "because of spark's lazy evaluation untill you call an action on that rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "103e59fc-5f25-4713-a3dc-e497a21d608b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark is awesome', 'spark is cool']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve all the elements of the rdd/dataFrame/dataset (from all nodes)\n",
    "stringRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e19d2be-bbc2-4d23-9af5-af720c368590",
   "metadata": {},
   "source": [
    ".collect() is an action \n",
    "\n",
    "as it name suggests it collects all the rows from each of the partitions in an rdd \n",
    "\n",
    "and brings them over to the driver program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c32cd9-ccb0-4112-9f38-eeb00ee57972",
   "metadata": {},
   "source": [
    "### the second way to create an RDD is \n",
    "\n",
    "### to read a dataset from a storage system which can be a \n",
    "\n",
    "### local computer file system\n",
    "\n",
    "### hdfs \n",
    "\n",
    "### cassandra\n",
    "\n",
    "### amazon s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0c3a52a-4127-470d-a799-fce06636d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read text file into rdd\n",
    "ratings = sc.textFile(\"./ratings.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2edc10d4-d431-4342-929a-793e688f5247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1::1193::5::978300760',\n",
       " '1::661::3::978302109',\n",
       " '1::914::3::978301968',\n",
       " '1::3408::4::978300275',\n",
       " '1::2355::5::978824291']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve all the elements of the rdd/dataFrame/dataset (from all nodes) and output first 5 rows\n",
    "ratings.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5971877b-b8dd-4d37-b98a-41fd9fa69807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1::1193::5::978300760',\n",
       " '1::661::3::978302109',\n",
       " '1::914::3::978301968',\n",
       " '1::3408::4::978300275',\n",
       " '1::2355::5::978824291']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the first 5 elements of the rdd\n",
    "ratings.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f32a94-a0ca-451a-8564-8ad83841f949",
   "metadata": {},
   "source": [
    "# transformations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b0b1c8-7aca-4b8c-844e-60e3c17fad38",
   "metadata": {},
   "source": [
    "transformations are operations on rdds that return a new rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aab9adc-ad43-401e-9cf3-11354011f92c",
   "metadata": {},
   "source": [
    "## map transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c624ecb-d91d-4d21-af2f-4f88efb07bd4",
   "metadata": {},
   "source": [
    "return a new rdd by applying a function to each element of this rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6797d3e-7a22-4fa4-b9ef-ac5ca43a11af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spark is awesome', 'spark is cool']\n",
      "['SPARK IS AWESOME', 'SPARK IS COOL']\n",
      "['SpArK Is aWeSoMe', 'SpArK Is cOoL']\n"
     ]
    }
   ],
   "source": [
    "# use the already created rdd and convert all letter to uppercase \n",
    "\n",
    "stringRDD_uppercase= stringRDD.map(lambda x: x.upper())\n",
    "\n",
    "# implement the function 'alternate_char_upper' which converts every other letter to upper case\n",
    "def alternate_char_upper(text):\n",
    "    new_text= []\n",
    "    for i, character in enumerate(text):\n",
    "        if i % 2 == 0:\n",
    "            new_text.append(character.upper())\n",
    "        else:\n",
    "            new_text.append(character)\n",
    "    return ''.join(new_text)\n",
    "\n",
    "stringRDD_alternate_uppercase= stringRDD.map(alternate_char_upper)\n",
    "\n",
    "print(stringRDD.collect())\n",
    "print(stringRDD_uppercase.collect())\n",
    "print(stringRDD_alternate_uppercase.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05d2bbf-399b-456c-956e-e3c8a24bbbcb",
   "metadata": {},
   "source": [
    "## flat map transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eccfb1d-dca4-4573-90f8-3967d97a372e",
   "metadata": {},
   "source": [
    "return a new rdd by first applying a function to all elements of this rdd and then flattening the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c078f71d-b122-428f-a240-8e0c1e773358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark', 'is', 'awesome', 'spark', 'is', 'cool']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the already created rdd and split the string at every ' ' character\n",
    "# using the flatMap transformation and the split function\n",
    "flatMap_Split= stringRDD.flatMap(lambda x: x.split(\" \"))\n",
    "\n",
    "flatMap_Split.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e921081c-867c-4529-a09f-8687b22f9825",
   "metadata": {},
   "source": [
    "## difference between map and flat map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e070d273-30d1-4b69-9c7b-d5135be81895",
   "metadata": {},
   "source": [
    "since the source rdd contains two strings the map transformation returns two separate objects (each with separate strings)\n",
    "\n",
    "the flat map transformation returns only one object with all separated strings from both input objects (strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea519a76-8066-4a41-93aa-5fc36c70cb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split using map transformation:\n",
      "[['spark', 'is', 'awesome'], ['spark', 'is', 'cool']]\n"
     ]
    }
   ],
   "source": [
    "print(\"split using map transformation:\")\n",
    "\n",
    "# use the already created rdd (stringRDD) and split the string at every ' ' character\n",
    "# using the map transformation and the split function\n",
    "map_Split= stringRDD.map(lambda x: x.split(\" \"))\n",
    "\n",
    "print(map_Split.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed354e89-3bb0-4a2b-b13b-68fc562e5c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split using flatmap transformation:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['spark', 'is', 'awesome', 'spark', 'is', 'cool']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"split using flatmap transformation:\")\n",
    "\n",
    "# use the already created rdd and split the string at every ' ' character\n",
    "# using the flatMap transformation and the split function\n",
    "flatMap_Split= stringRDD.flatMap(lambda x: x.split(\" \"))\n",
    "\n",
    "flatMap_Split.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e4a18f-031a-4be6-98c1-6d6582a89116",
   "metadata": {},
   "source": [
    "## filter transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfc4808-10d5-4fe7-83d6-1faf2db4cebc",
   "metadata": {},
   "source": [
    "return a new rdd containing only the elements that satisfy a predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2f7f37f-43b5-4e0e-8f6e-1c830dcf3a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark is awesome']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter all objects from rdd containing 'awesome'\n",
    "awesomeLineRDD = stringRDD.filter(lambda x: \"awesome\" in x)\n",
    "awesomeLineRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1f7951e-92b4-41ab-802a-6b19473cc643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark is awesome', 'spark is cool']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter all objects from rdd containing 'spark'\n",
    "sparkLineRDD = stringRDD.filter(lambda x: \"spark\" in x)\n",
    "sparkLineRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08edaf1-4273-4f21-9097-b6bc86e9b60c",
   "metadata": {},
   "source": [
    "## union transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f16ceb-241a-42ae-849e-8b3180225e73",
   "metadata": {},
   "source": [
    "return a new rdd containing all items from two original rdds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ebe6450-29aa-43f5-9471-a3aedc288cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 1, 2, 4, 6, 7, 8]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create two new rdds\n",
    "rdd1 = sc.parallelize([1,2,3,4,5])\n",
    "rdd2 = sc.parallelize([1,2,4,6,7,8])\n",
    "\n",
    "# create a third rdd with 'union' transformation on rdd1 and rdd2\n",
    "rdd3 = rdd1.union(rdd2)\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1da704-bd67-4c36-8920-b3a91907e61d",
   "metadata": {},
   "source": [
    "## intersection transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8552f5-8ad3-4228-a0d6-0d83f061b6f6",
   "metadata": {},
   "source": [
    "return the intersection of this rdd and another one\n",
    "\n",
    "the output will not contain any duplicate elements even if the input rdds did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4eb2e3f1-2c2d-4906-a2bc-8b178e1255da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 1]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create two new rdds\n",
    "rdd1 = sc.parallelize([1,2,3,4,5])\n",
    "rdd2 = sc.parallelize([1,2,4,6,7,8])\n",
    "\n",
    "# create a third rdd with 'intersection' transformation on rdd1 and rdd2\n",
    "rdd3 = rdd1.intersection(rdd2)\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce4b9b5-a458-404f-9f3e-7a0728de51df",
   "metadata": {},
   "source": [
    "## substract tranformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2825e0-f386-4f18-9dbb-0cfb4ce558d1",
   "metadata": {},
   "source": [
    "return each value in self that is not contained in other (return a new dataframe containing rows in this DataFrame but not in another dataframe) \n",
    "\n",
    "this is equivalent to EXCEPT DISTINCT in sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43bbb57c-e6a6-4e8e-8142-4b530bc00d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['about', 'learn', 'amazing', 'thing', 'spark', 'very', 'simple']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new rdd 'words' use transformation flatMap and map ... all in one line\n",
    "words = sc.parallelize([\"the amazing thing about spark is that it is very simple to learn\"]).flatMap(lambda x: x.split(\" \"))\n",
    "\n",
    "# create a new rdd 'stopWords'\n",
    "stopWords = sc.parallelize([\"the\", \"it\", \"is\", \"to\", \"that\", ''])\n",
    "\n",
    "# use substract transformation on words rdd\n",
    "realWords = words.subtract(stopWords)\n",
    "realWords.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1565744c-00c9-42ef-a5b8-f9207855579a",
   "metadata": {},
   "source": [
    "## distinct transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef0ad75-bf55-4e89-b106-517b5a868a7d",
   "metadata": {},
   "source": [
    "return a new rdd containing distinct items from the original rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86203aa3-2272-4a59-91ef-ee325044e074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new rdd 'duplicateValueRDD'\n",
    "duplicateValueRDD = sc.parallelize([1,1,2,2,3,3])\n",
    "\n",
    "# use distinct transformation on rdd and collect action - in one line\n",
    "duplicateValueRDD.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a11387d-65bb-4db5-93f0-aaf4e08aef29",
   "metadata": {},
   "source": [
    "## sample transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796baf5b-bf6b-4e68-8bc2-34ecc2a3012b",
   "metadata": {},
   "source": [
    "return a new rdd containing a statistical sample of the original rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9e822d45-1eff-46ee-91f5-7bf0f4a00953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new rdd 'numbers'\n",
    "# the second parameter of the parallelize transformation is optional integer value\n",
    "# and defines the number of partitions the data would be parallelized to\n",
    "numbers = sc.parallelize([1,2,3,4,5,6,7,8,9,10], 2)\n",
    "\n",
    "# the transformation 'sample' returns a sampled subset of the numbers rdd\n",
    "numbers.sample(True, 0.3).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3038e90d-883f-474a-b115-51dc794bd0fd",
   "metadata": {},
   "source": [
    "## groupby transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42e1b4e-7d3f-494e-a7df-a42d8c985203",
   "metadata": {},
   "source": [
    "group the data in the original rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5485869e-8ab1-45c5-9a23-a018ae2f9521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('j', ['john', 'james']), ('f', ['fred']), ('a', ['anna'])]\n"
     ]
    }
   ],
   "source": [
    "#create pairs where the key is the output of a user function and the value is all items for which the function yields this key\n",
    "\n",
    "# create a new rdd 'x'\n",
    "x = sc.parallelize(['john', 'fred', 'anna', 'james'])\n",
    "\n",
    "# groupBy all elements by the first letter of each element (which will be used as keys)\n",
    "y = x.groupBy(lambda w: w[0])\n",
    "\n",
    "# 'loop' through all element of the 'y' rdd and print the objects\n",
    "print([(k, list(v)) for (k, v) in y.collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "02d198a8-9329-41ba-a64b-4907a186d4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 5), ('b', 4), ('a', 3), ('a', 2), ('a', 1)]\n",
      "[('b', [5, 4]), ('a', [3, 2, 1])]\n"
     ]
    }
   ],
   "source": [
    "# create a new pair where the original key corresponds to this collected group of values\n",
    "\n",
    "# create new 'x' rdd with key value pairs\n",
    "x = sc.parallelize([('b',5),('b',4),('a',3),('a',2),('a',1)])\n",
    "\n",
    "# create rdd 'y' using the groupBy transformation on the keys of rdd 'x'\n",
    "y = x.groupByKey()\n",
    "\n",
    "# print objects of RDD 'x'\n",
    "print(x.collect())\n",
    "\n",
    "# 'loop' through all element of the 'y' rdd and print the objects\n",
    "print(list((j[0], list(j[1])) for j in y.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6705f9ea-90dd-482e-96d8-4c7adc8b548c",
   "metadata": {},
   "source": [
    "## map partitions transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2868b7-9b60-4112-9275-ce17a8ba4f89",
   "metadata": {},
   "source": [
    "return a new rdd by applying a function to each partition of this rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b7fb511f-1ffa-4436-89d6-82d9d98aa135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 42, 5, 42]\n",
      "[[1], [2, 3]]\n",
      "[[1, 42], [5, 42]]\n"
     ]
    }
   ],
   "source": [
    "# create new rdd with two partitions\n",
    "x = sc.parallelize([1,2,3], 2)\n",
    "\n",
    "# define the function 'f' - it is an iterable\n",
    "# the function sums all values of a partition and returns the sum and the number 42 as an object\n",
    "def f(iterator): yield sum(iterator); yield 42\n",
    "    \n",
    "# use the transformation 'mapPartitions' with the function 'f'\n",
    "y = x.mapPartitions(f)\n",
    "\n",
    "# glom() flattens elements on the same partition\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a0123d-b0e4-497e-8db9-7cfcedd2498f",
   "metadata": {},
   "source": [
    "## map partition with index transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f41d102-06f3-49e3-b1b7-8d57dddb5ca1",
   "metadata": {},
   "source": [
    "return a new rdd by applying a function to each partition of this rdd \n",
    "\n",
    "while tracking the index of the original partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7870d5b-cd60-45b6-ac85-a565164a1350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[(0, 1), (1, 5)]\n",
      "[[1], [2, 3]]\n",
      "[[(0, 1)], [(1, 5)]]\n"
     ]
    }
   ],
   "source": [
    "# create new rdd with two partitions\n",
    "x = sc.parallelize([1,2,3], 2)\n",
    "\n",
    "# define the function 'f' - it is an iterable\n",
    "# the function sums all values of a partition and returns the index of the original partition and the sum as an object\n",
    "def f(partitionIndex, iterator): yield (partitionIndex, sum(iterator))\n",
    "    \n",
    "# use the transformation 'mapPartitions' with the function 'f'\n",
    "y = x.mapPartitionsWithIndex(f)\n",
    "\n",
    "# glom() flattens elements on the same partition\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e447b-7ab4-48a7-92be-f59f0cfb60a4",
   "metadata": {},
   "source": [
    "## join transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d520c550-c5fb-482e-97d7-b9d0b358e7ff",
   "metadata": {},
   "source": [
    "return a new rdd containing all pairs of elements having the same key in the original rdds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db48efe-f897-47dd-b110-b25287b4a200",
   "metadata": {},
   "source": [
    "union(otherRDD,numPartitions=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "264c3370-a7e3-4548-8bf7-098e00273336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', (2, 5)), ('a', (1, 3)), ('a', (1, 4))]\n",
      "[('b', (5, 2)), ('a', (3, 1)), ('a', (4, 1))]\n"
     ]
    }
   ],
   "source": [
    "# create rdd 'x'\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 2)])\n",
    "\n",
    "# create rdd 'y'\n",
    "y = sc.parallelize([(\"a\", 3), (\"a\", 4), (\"b\", 5)])\n",
    "\n",
    "# create rdd 'z1' as a join result on keys from rdd 'x' and 'y'\n",
    "z1 = x.join(y)\n",
    "print(z1.collect())\n",
    "\n",
    "# create rdd 'z2' as a join result on keys from rdd 'y' and 'x'\n",
    "z2 = y.join(x)\n",
    "print(z2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45daba10-6ebf-43b6-b557-0c73f4b9f35f",
   "metadata": {},
   "source": [
    "## coalesce transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26177ec-0d26-4efc-bea5-e94b955a4f4b",
   "metadata": {},
   "source": [
    "return a new rdd which is reduced to a smaller number of partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7336f304-af05-41d2-852b-685606363b42",
   "metadata": {},
   "source": [
    "coalesce(numPartitions,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "243d55cd-f33d-496d-ba02-84df19279024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3], [4, 5]]\n",
      "[[1], [2, 3, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "# create a rdd with three partition\n",
    "x = sc.parallelize([1, 2, 3, 4, 5], 3)\n",
    "\n",
    "# reduce the number of partitions to two by using the coalesce transformation\n",
    "y = x.coalesce(2)\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc0216f-995a-49da-9609-0f6bfc0d0d69",
   "metadata": {},
   "source": [
    "## keyby transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd8045-2bed-48df-ad1a-7f707554fedc",
   "metadata": {},
   "source": [
    "create a pair rdd forming one pair for each item in the original rdd\n",
    "\n",
    "the pair’s key is calculated from the value via a user-supplied function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c197a18c-6dbc-497a-a122-f73a2bb2b0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['john', 'fred', 'anna', 'james']\n",
      "[('j', 'john'), ('f', 'fred'), ('a', 'anna'), ('j', 'james')]\n"
     ]
    }
   ],
   "source": [
    "# create a new rdd 'x'\n",
    "x = sc.parallelize(['john', 'fred', 'anna', 'james'])\n",
    "\n",
    "# use the first letter of each element as the key for the element\n",
    "y = x.keyBy(lambda w: w[0])\n",
    "\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0278b6f0-4002-4fec-a636-c1272a7d98c7",
   "metadata": {},
   "source": [
    "## partitionby transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8b1c71-24ef-49a4-a9c0-09ec41f7c494",
   "metadata": {},
   "source": [
    "return a new rdd with the specified number of partitions placing original items into the partition returned by a user supplied function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f75bc1f-b720-4cbf-90e7-3be4679b9e51",
   "metadata": {},
   "source": [
    "partitionBy(numPartitions,partitioner=portable_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "787461e7-6c4f-45b5-bd63-a1d35fac19c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('j', 'james')], [('f', 'fred')], [('a', 'anna'), ('j', 'john')]]\n",
      "[[('f', 'fred'), ('a', 'anna')], [('j', 'james'), ('j', 'john')]]\n"
     ]
    }
   ],
   "source": [
    "# create a rdd with three partition\n",
    "x = sc.parallelize([('j','james'),('f','fred'),('a','anna'),('j','john')], 3)\n",
    "\n",
    "# creta a new RDD 'y' with only two partitions and place each item in partition 0 if the first letter of the item is < 'h'\n",
    "# the item will be placed in partition 1 otherwise \n",
    "y = x.partitionBy(2, lambda w: 0 if w[0] < 'h' else 1)\n",
    "\n",
    "# glom() flattens elements on the same partition\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5961f375-225a-4807-9d22-50702e880167",
   "metadata": {},
   "source": [
    "## zip transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c1664-a8e0-4b38-ae89-fedec3950e51",
   "metadata": {},
   "source": [
    "return a new rdd containing pairs whose key is the item in the original rdd and whose value is that item’s corresponding element (same partition same index) in a second rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf98b38-03d2-4ecf-980b-038b9d047c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myproject_kernel",
   "language": "python",
   "name": "myproject_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
