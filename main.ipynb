{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e961f7d-1215-43eb-ad89-ec3c66ff41d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries from pyspark \n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# set values for Spark configuration\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"Tutorial\")\n",
    "\n",
    "# get (if already running) or create a Spark Context\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25bf4481-43c1-4e18-8227-57bee9800c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n",
      "('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.app.submitTime', '1739025194051')\n",
      "('spark.driver.host', 'mac.modem.local')\n",
      "('spark.app.startTime', '1739025194155')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.app.name', 'TestApp')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.app.id', 'local-1739025194461')\n",
      "('spark.driver.port', '50098')\n",
      "('spark.ui.showConsoleProgress', 'true')\n"
     ]
    }
   ],
   "source": [
    "# check (try) if SparkContext variable (sc) exists and print information about the SparkContext\n",
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    print(\"please create SparkContext first (run cell above)\")\n",
    "else:\n",
    "    configurations = sc.getConf().getAll()\n",
    "    for item in configurations: print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a0b7dda-5b20-4f8c-8b32-561d456c79c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mac.modem.local:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TestApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=TestApp>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da9708c-f0e0-49c3-97f6-9eb9142ddb71",
   "metadata": {},
   "source": [
    "# creating rdds:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f27feb5-36f7-4566-9b30-5e5cfc0a0cba",
   "metadata": {},
   "source": [
    "### the first way to create an rdd\n",
    "\n",
    "### is to parallelize an python object\n",
    "\n",
    "### meaning converting it to a distributed dataset that can be operated in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1fffc873-331e-48e9-a25e-df52b5d3d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of strings\n",
    "stringList = [\"spark is awesome\",\"spark is cool\"]\n",
    "\n",
    "# convert list of strings into a spark rdd\n",
    "stringRDD = sc.parallelize(stringList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b6b4cc0-b6e7-4fcb-b402-ffbffa6c8106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[11] at readRDDFromFile at PythonRDD.scala:289"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output rdd information\n",
    "stringRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732e219a-d2fa-4ea2-b8eb-6ea4a7b10f90",
   "metadata": {},
   "source": [
    "one thing to notice is that you are not able to see the output\n",
    "\n",
    "because of spark's lazy evaluation untill you call an action on that rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "103e59fc-5f25-4713-a3dc-e497a21d608b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark is awesome', 'spark is cool']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve all the elements of the rdd/dataFrame/dataset (from all nodes)\n",
    "stringRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e19d2be-bbc2-4d23-9af5-af720c368590",
   "metadata": {},
   "source": [
    ".collect() is an action \n",
    "\n",
    "as it name suggests it collects all the rows from each of the partitions in an rdd \n",
    "\n",
    "and brings them over to the driver program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c32cd9-ccb0-4112-9f38-eeb00ee57972",
   "metadata": {},
   "source": [
    "### the second way to create an RDD is \n",
    "\n",
    "### to read a dataset from a storage system which can be a \n",
    "\n",
    "### local computer file system\n",
    "\n",
    "### hdfs \n",
    "\n",
    "### cassandra\n",
    "\n",
    "### amazon s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0c3a52a-4127-470d-a799-fce06636d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read text file into rdd\n",
    "ratings = sc.textFile(\"./ratings.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2edc10d4-d431-4342-929a-793e688f5247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1::1193::5::978300760',\n",
       " '1::661::3::978302109',\n",
       " '1::914::3::978301968',\n",
       " '1::3408::4::978300275',\n",
       " '1::2355::5::978824291']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve all the elements of the rdd/dataFrame/dataset (from all nodes) and output first 5 rows\n",
    "ratings.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5971877b-b8dd-4d37-b98a-41fd9fa69807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1::1193::5::978300760',\n",
       " '1::661::3::978302109',\n",
       " '1::914::3::978301968',\n",
       " '1::3408::4::978300275',\n",
       " '1::2355::5::978824291']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the first 5 elements of the rdd\n",
    "ratings.take(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myproject_kernel",
   "language": "python",
   "name": "myproject_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
